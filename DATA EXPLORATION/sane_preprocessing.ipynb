{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    'hash_encoding': {\n",
    "        'num_levels': 16,\n",
    "        'level_dim': 2,\n",
    "        'input_dim': 3,\n",
    "        'log2_hashmap_size': 19,\n",
    "        'base_resolution': 16\n",
    "    },\n",
    "    'mlp': {\n",
    "        'num_layers': 3,  # Number of layers in geometric MLP\n",
    "        'hidden_dim': 64,  # Hidden dimension size\n",
    "    }\n",
    "}\n",
    "\n",
    "def load_torch_weights(file_path):\n",
    "    \"\"\"Load model weights from a checkpoint file.\"\"\"\n",
    "    try:\n",
    "        weights = torch.load(file_path, map_location='cpu')\n",
    "        return weights['model']\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading file {file_path}: {e}\")\n",
    "        return None\n",
    "    \n",
    "model_path_base = 'shared_data/CarrotKhanStatue/base_000_000_000/checkpoints/final.pth'\n",
    "model_path_x_180 = 'shared_data/CarrotKhanStatue/x_180_000_000/checkpoints/final.pth'\n",
    "model_path_test = 'shared_data/GoldBag/base_000_000_000/checkpoints/final.pth'\n",
    "\n",
    "\n",
    "nerf_base = load_torch_weights(model_path_base)\n",
    "nerf_x_180 = load_torch_weights(model_path_x_180)\n",
    "nerf_test = load_torch_weights(model_path_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hash_encoding_structure(model_weights, num_levels=16, level_dim=2, input_dim=3, log2_hashmap_size=19, base_resolution=16):\n",
    "    \"\"\"\n",
    "    Extract and organize hash encoding weights into hierarchical structure.\n",
    "    \n",
    "    Args:\n",
    "        model_weights (dict): The loaded model weights dictionary\n",
    "        num_levels (int): Number of levels in hash encoding\n",
    "        level_dim (int): Dimension of encoding at each level\n",
    "        input_dim (int): Input dimension (typically 3 for 3D)\n",
    "        log2_hashmap_size (int): Log2 of maximum hash table size\n",
    "        base_resolution (int): Base resolution of the grid\n",
    "        \n",
    "    Returns:\n",
    "        dict: Hierarchical structure of hash encoding weights\n",
    "    \"\"\"\n",
    "    # Extract hash encoding embeddings\n",
    "    embeddings = model_weights['_orig_mod.grid_encoder.embeddings']\n",
    "    \n",
    "    # Calculate per-level parameters\n",
    "    max_params = 2 ** log2_hashmap_size\n",
    "    per_level_scale = np.exp2(np.log2(2048 / base_resolution) / (num_levels - 1))\n",
    "    \n",
    "    # Initialize structure to store weights\n",
    "    hash_structure = {}\n",
    "    offset = 0\n",
    "    \n",
    "    for level in range(num_levels):\n",
    "        # Calculate resolution at this level\n",
    "        resolution = int(np.ceil(base_resolution * (per_level_scale ** level)))\n",
    "        \n",
    "        # Calculate number of parameters for this level\n",
    "        params_in_level = min(max_params, (resolution) ** input_dim)\n",
    "        params_in_level = int(np.ceil(params_in_level / 8) * 8)  # make divisible by 8\n",
    "        \n",
    "        # Extract weights for this level\n",
    "        level_weights = embeddings[offset:offset + params_in_level]\n",
    "        \n",
    "        # Store level information\n",
    "        hash_structure[f'level_{level}'] = {\n",
    "            'resolution': resolution,\n",
    "            'num_params': params_in_level,\n",
    "            'weights': level_weights,\n",
    "            'weights_shape': level_weights.shape,\n",
    "            'scale': per_level_scale ** level\n",
    "        }\n",
    "        \n",
    "        offset += params_in_level\n",
    "    \n",
    "    # Add global information\n",
    "    hash_structure['global_info'] = {\n",
    "        'total_params': offset,\n",
    "        'embedding_dim': level_dim,\n",
    "        'base_resolution': base_resolution,\n",
    "        'max_resolution': int(np.ceil(base_resolution * (per_level_scale ** (num_levels-1)))),\n",
    "        'per_level_scale': per_level_scale\n",
    "    }\n",
    "    \n",
    "    return hash_structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrhe_by_layer_base = extract_hash_encoding_structure(nerf_base)\n",
    "\n",
    "mrhe_by_layer_x_180 = extract_hash_encoding_structure(nerf_x_180)\n",
    "\n",
    "mrhe_by_layer_nerf_test = extract_hash_encoding_structure(nerf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dict = {layer: info['weights'] for layer, info in mrhe_by_layer_base.items() if layer.startswith('level_')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'level_0': tensor([[-0.0902, -0.0635],\n",
       "         [ 0.1001,  0.1045],\n",
       "         [ 0.1441,  0.0420],\n",
       "         ...,\n",
       "         [ 0.2313,  0.1918],\n",
       "         [ 0.1921,  0.0209],\n",
       "         [-0.0819, -0.0145]]),\n",
       " 'level_1': tensor([[-0.0154,  0.0137],\n",
       "         [-0.0872,  0.0826],\n",
       "         [-0.0383,  0.1021],\n",
       "         ...,\n",
       "         [-0.8903,  0.2423],\n",
       "         [ 0.0768, -0.0400],\n",
       "         [-0.1069,  0.0990]]),\n",
       " 'level_2': tensor([[-0.0608,  0.5266],\n",
       "         [-0.4926,  0.6712],\n",
       "         [-0.3840,  0.9586],\n",
       "         ...,\n",
       "         [-0.2585, -0.2453],\n",
       "         [-0.0279, -0.0033],\n",
       "         [ 0.3932,  0.5994]]),\n",
       " 'level_3': tensor([[-0.3374, -0.3020],\n",
       "         [-0.4407, -0.8653],\n",
       "         [ 0.1957, -0.4545],\n",
       "         ...,\n",
       "         [-0.5310, -0.8185],\n",
       "         [-0.6661, -0.0695],\n",
       "         [ 0.1364,  0.2074]]),\n",
       " 'level_4': tensor([[ 2.4145e-01, -1.7465e-01],\n",
       "         [ 8.2795e-01,  1.2799e+00],\n",
       "         [ 1.7449e+00,  1.3036e+00],\n",
       "         ...,\n",
       "         [-2.8503e-05, -3.0760e-05],\n",
       "         [-1.3017e-05,  9.0734e-06],\n",
       "         [-4.9367e-05,  8.6881e-06]]),\n",
       " 'level_5': tensor([[ 4.0890e-05,  7.6951e-05],\n",
       "         [-9.5460e-05,  6.1019e-05],\n",
       "         [ 1.2033e-06,  5.2361e-06],\n",
       "         ...,\n",
       "         [ 3.8781e-01,  5.9099e-01],\n",
       "         [ 6.5112e-01,  9.0075e-01],\n",
       "         [ 5.4625e-01,  7.8741e-01]]),\n",
       " 'level_6': tensor([[ 0.2590,  0.7776],\n",
       "         [ 0.7114,  0.4439],\n",
       "         [-0.2757,  0.1138],\n",
       "         ...,\n",
       "         [ 1.0859,  0.9500],\n",
       "         [ 0.1458, -0.2405],\n",
       "         [-0.9411, -0.9660]]),\n",
       " 'level_7': tensor([[-0.3449, -0.4059],\n",
       "         [-0.4190, -0.5039],\n",
       "         [-0.0210, -0.0045],\n",
       "         ...,\n",
       "         [-1.4542,  1.1772],\n",
       "         [ 0.7237, -0.8163],\n",
       "         [ 0.4331, -0.6143]]),\n",
       " 'level_8': tensor([[ 0.3137, -0.3452],\n",
       "         [ 0.2850, -0.2764],\n",
       "         [ 0.5422, -0.8677],\n",
       "         ...,\n",
       "         [-0.0681,  0.5474],\n",
       "         [-0.0939,  1.1242],\n",
       "         [-1.3221,  1.0961]]),\n",
       " 'level_9': tensor([[-0.5514,  1.0340],\n",
       "         [-0.4821,  0.2746],\n",
       "         [ 0.4318, -0.5979],\n",
       "         ...,\n",
       "         [ 0.5781, -0.5761],\n",
       "         [ 0.1912, -0.5656],\n",
       "         [ 0.3412, -0.4954]]),\n",
       " 'level_10': tensor([[-0.2489,  1.5422],\n",
       "         [-0.1109, -0.5272],\n",
       "         [-1.1021, -0.4001],\n",
       "         ...,\n",
       "         [ 0.9345,  0.5289],\n",
       "         [-2.3923, -0.6024],\n",
       "         [-1.0652, -0.8446]]),\n",
       " 'level_11': tensor([[ 0.5084,  0.1046],\n",
       "         [ 0.0426,  0.2166],\n",
       "         [ 0.7701,  0.4564],\n",
       "         ...,\n",
       "         [-1.3001, -1.0185],\n",
       "         [-1.1733, -0.9048],\n",
       "         [-0.0184,  0.1307]]),\n",
       " 'level_12': tensor([[-0.5893, -0.2484],\n",
       "         [ 0.0757,  0.1081],\n",
       "         [-0.4799, -0.3933],\n",
       "         ...,\n",
       "         [-1.3671, -0.9401],\n",
       "         [ 0.8162, -0.0159],\n",
       "         [ 0.0779,  0.2524]]),\n",
       " 'level_13': tensor([[ 0.3181, -0.9238],\n",
       "         [ 0.0972,  0.3173],\n",
       "         [-1.1821, -0.0694],\n",
       "         ...,\n",
       "         [ 0.7146, -1.5344],\n",
       "         [-0.2136, -0.2135],\n",
       "         [-0.7239, -0.6050]]),\n",
       " 'level_14': tensor([[-1.3405,  1.0017],\n",
       "         [-0.8184,  0.5605],\n",
       "         [ 0.7304, -0.4010],\n",
       "         ...,\n",
       "         [-1.1059,  1.0039],\n",
       "         [ 0.0955,  0.3888],\n",
       "         [-0.0719,  1.1246]]),\n",
       " 'level_15': tensor([[-0.0584,  0.0354],\n",
       "         [ 0.6577, -0.1150],\n",
       "         [ 0.3534,  0.4464],\n",
       "         ...,\n",
       "         [-1.0207, -0.4390],\n",
       "         [-1.5475,  0.4206],\n",
       "         [-1.0455,  0.3467]])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turned 180 CarrotKhanStatue\n",
    "x_180_dict = {layer: info['weights'] for layer, info in mrhe_by_layer_x_180.items() if layer.startswith('level_')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# different nerf to check back (currently Goldback)\n",
    "test_dict = {layer: info['weights'] for layer, info in mrhe_by_layer_nerf_test.items() if layer.startswith('level_')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "positions = []\n",
    "global_index = 0  # zählt alle Tokens über alle Layer hinweg\n",
    "\n",
    "for key in sorted(base_dict.keys(), key=lambda x: int(x.split(\"_\")[1])):\n",
    "    layer_index = int(key.split(\"_\")[1])              # z.B. 'level_3' → 3\n",
    "    layer_tensor = base_dict[key]                     # Tensor der Form [N, 2]\n",
    "    num_tokens_in_layer = layer_tensor.shape[0]\n",
    "\n",
    "    for position_in_layer in range(num_tokens_in_layer):\n",
    "        token = layer_tensor[position_in_layer]        # Ein Vektor [2]\n",
    "        tokens.append(token)\n",
    "        positions.append(torch.tensor([global_index, layer_index, position_in_layer]))\n",
    "        global_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = torch.stack(tokens)        # → Shape: [N, 2]\n",
    "positions = torch.stack(positions)  # → Shape: [N, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6098120, 2])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[      0,       0,       0],\n",
       "        [      1,       0,       1],\n",
       "        [      2,       0,       2],\n",
       "        ...,\n",
       "        [6098117,      15,  524285],\n",
       "        [6098118,      15,  524286],\n",
       "        [6098119,      15,  524287]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datasc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
