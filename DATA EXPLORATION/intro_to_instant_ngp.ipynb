{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3771028/369776395.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  weights = torch.load(file_path, map_location='cpu')\n"
     ]
    }
   ],
   "source": [
    "CONFIG = {\n",
    "    'hash_encoding': {\n",
    "        'num_levels': 16,\n",
    "        'level_dim': 2,\n",
    "        'input_dim': 3,\n",
    "        'log2_hashmap_size': 19,\n",
    "        'base_resolution': 16\n",
    "    },\n",
    "    'mlp': {\n",
    "        'num_layers': 3,  # Number of layers in geometric MLP\n",
    "        'hidden_dim': 64,  # Hidden dimension size\n",
    "    }\n",
    "}\n",
    "\n",
    "def load_torch_weights(file_path):\n",
    "    \"\"\"Load model weights from a checkpoint file.\"\"\"\n",
    "    try:\n",
    "        weights = torch.load(file_path, map_location='cpu')\n",
    "        return weights['model']\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading file {file_path}: {e}\")\n",
    "        return None\n",
    "    \n",
    "model_path = 'shared_data/CarrotKhanStatue/base_000_000_000/checkpoints/final.pth'\n",
    "nerf = load_torch_weights(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_orig_mod.aabb_train                     at nerf[k] you find a <class 'torch.Tensor'>\n",
      "_orig_mod.aabb_infer                     at nerf[k] you find a <class 'torch.Tensor'>\n",
      "_orig_mod.density_grid                   at nerf[k] you find a <class 'torch.Tensor'>\n",
      "_orig_mod.density_bitfield               at nerf[k] you find a <class 'torch.Tensor'>\n",
      "_orig_mod.grid_encoder.embeddings        at nerf[k] you find a <class 'torch.Tensor'>\n",
      "_orig_mod.grid_encoder.offsets           at nerf[k] you find a <class 'torch.Tensor'>\n",
      "_orig_mod.grid_mlp.net.0.weight          at nerf[k] you find a <class 'torch.Tensor'>\n",
      "_orig_mod.grid_mlp.net.1.weight          at nerf[k] you find a <class 'torch.Tensor'>\n",
      "_orig_mod.grid_mlp.net.2.weight          at nerf[k] you find a <class 'torch.Tensor'>\n",
      "_orig_mod.view_mlp.net.0.weight          at nerf[k] you find a <class 'torch.Tensor'>\n",
      "_orig_mod.view_mlp.net.1.weight          at nerf[k] you find a <class 'torch.Tensor'>\n",
      "_orig_mod.view_mlp.net.2.weight          at nerf[k] you find a <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "for k in nerf.keys():\n",
    "    print(f\"{k}\".ljust(40) + f\" at nerf[k] you find a {type(nerf[k])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hash_encoding_structure(model_weights, num_levels=16, level_dim=2, input_dim=3, log2_hashmap_size=19, base_resolution=16):\n",
    "    \"\"\"\n",
    "    Extract and organize hash encoding weights into hierarchical structure.\n",
    "    \n",
    "    Args:\n",
    "        model_weights (dict): The loaded model weights dictionary\n",
    "        num_levels (int): Number of levels in hash encoding\n",
    "        level_dim (int): Dimension of encoding at each level\n",
    "        input_dim (int): Input dimension (typically 3 for 3D)\n",
    "        log2_hashmap_size (int): Log2 of maximum hash table size\n",
    "        base_resolution (int): Base resolution of the grid\n",
    "        \n",
    "    Returns:\n",
    "        dict: Hierarchical structure of hash encoding weights\n",
    "    \"\"\"\n",
    "    # Extract hash encoding embeddings\n",
    "    embeddings = model_weights['_orig_mod.grid_encoder.embeddings']\n",
    "    \n",
    "    # Calculate per-level parameters\n",
    "    max_params = 2 ** log2_hashmap_size\n",
    "    per_level_scale = np.exp2(np.log2(2048 / base_resolution) / (num_levels - 1))\n",
    "    \n",
    "    # Initialize structure to store weights\n",
    "    hash_structure = {}\n",
    "    offset = 0\n",
    "    \n",
    "    for level in range(num_levels):\n",
    "        # Calculate resolution at this level\n",
    "        resolution = int(np.ceil(base_resolution * (per_level_scale ** level)))\n",
    "        \n",
    "        # Calculate number of parameters for this level\n",
    "        params_in_level = min(max_params, (resolution) ** input_dim)\n",
    "        params_in_level = int(np.ceil(params_in_level / 8) * 8)  # make divisible by 8\n",
    "        \n",
    "        # Extract weights for this level\n",
    "        level_weights = embeddings[offset:offset + params_in_level]\n",
    "        \n",
    "        # Store level information\n",
    "        hash_structure[f'level_{level}'] = {\n",
    "            'resolution': resolution,\n",
    "            'num_params': params_in_level,\n",
    "            'weights': level_weights,\n",
    "            'weights_shape': level_weights.shape,\n",
    "            'scale': per_level_scale ** level\n",
    "        }\n",
    "        \n",
    "        offset += params_in_level\n",
    "    \n",
    "    # Add global information\n",
    "    hash_structure['global_info'] = {\n",
    "        'total_params': offset,\n",
    "        'embedding_dim': level_dim,\n",
    "        'base_resolution': base_resolution,\n",
    "        'max_resolution': int(np.ceil(base_resolution * (per_level_scale ** (num_levels-1)))),\n",
    "        'per_level_scale': per_level_scale\n",
    "    }\n",
    "    \n",
    "    return hash_structure\n",
    "\n",
    "mrhe_by_layer = extract_hash_encoding_structure(nerf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level_0 Resolution: 16 \t\tShape of Hash Layer Params:torch.Size([4096, 2])\n",
      "level_1 Resolution: 23 \t\tShape of Hash Layer Params:torch.Size([12168, 2])\n",
      "level_2 Resolution: 31 \t\tShape of Hash Layer Params:torch.Size([29792, 2])\n",
      "level_3 Resolution: 43 \t\tShape of Hash Layer Params:torch.Size([79512, 2])\n",
      "level_4 Resolution: 59 \t\tShape of Hash Layer Params:torch.Size([205384, 2])\n",
      "level_5 Resolution: 81 \t\tShape of Hash Layer Params:torch.Size([524288, 2])\n",
      "level_6 Resolution: 112 \t\tShape of Hash Layer Params:torch.Size([524288, 2])\n",
      "level_7 Resolution: 154 \t\tShape of Hash Layer Params:torch.Size([524288, 2])\n",
      "level_8 Resolution: 213 \t\tShape of Hash Layer Params:torch.Size([524288, 2])\n",
      "level_9 Resolution: 295 \t\tShape of Hash Layer Params:torch.Size([524288, 2])\n",
      "level_10 Resolution: 407 \t\tShape of Hash Layer Params:torch.Size([524288, 2])\n",
      "level_11 Resolution: 562 \t\tShape of Hash Layer Params:torch.Size([524288, 2])\n",
      "level_12 Resolution: 777 \t\tShape of Hash Layer Params:torch.Size([524288, 2])\n",
      "level_13 Resolution: 1073 \t\tShape of Hash Layer Params:torch.Size([524288, 2])\n",
      "level_14 Resolution: 1483 \t\tShape of Hash Layer Params:torch.Size([524288, 2])\n",
      "level_15 Resolution: 2048 \t\tShape of Hash Layer Params:torch.Size([524288, 2])\n",
      "Total MRHE Table Params: 12196240\n"
     ]
    }
   ],
   "source": [
    "tmo = 0\n",
    "for k in mrhe_by_layer.keys():\n",
    "    if 'level' in k:\n",
    "        print(k, \"Resolution: \" + str( mrhe_by_layer[k]['resolution']), \"\\t\\tShape of Hash Layer Params:\" + str(mrhe_by_layer[k]['weights'].shape))\n",
    "        tmo += mrhe_by_layer[k]['weights'].shape[0]*mrhe_by_layer[k]['weights'].shape[1]\n",
    "print(f\"Total MRHE Table Params: \" + str(tmo))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geometry MLP layers: 3\n",
      "View MLP layers: 3\n",
      "\n",
      "Geometry MLP layer shapes:\n",
      "  layer_0: torch.Size([64, 32]) - Input: 32, Output: 64\n",
      "  layer_1: torch.Size([64, 64]) - Input: 64, Output: 64\n",
      "  layer_2: torch.Size([16, 64]) - Input: 64, Output: 16\n",
      "\n",
      "View MLP layer shapes:\n",
      "  layer_0: torch.Size([32, 31]) - Input: 31, Output: 32\n",
      "  layer_1: torch.Size([32, 32]) - Input: 32, Output: 32\n",
      "  layer_2: torch.Size([3, 32]) - Input: 32, Output: 3\n"
     ]
    }
   ],
   "source": [
    "def extract_mlp_weights(model_weights):\n",
    "    \"\"\"Extract geometric and view-dependent MLP weights from the model.\"\"\"\n",
    "    geometry_layers = {}\n",
    "    view_mlp_layers = {}\n",
    "    \n",
    "    # Extract geometry MLP weights\n",
    "    for i in range(CONFIG['mlp']['num_layers']):\n",
    "        weight_key = f'_orig_mod.grid_mlp.net.{i}.weight'\n",
    "        bias_key = f'_orig_mod.grid_mlp.net.{i}.bias'\n",
    "        \n",
    "        if weight_key in model_weights:\n",
    "            geometry_layers[f'layer_{i}'] = {\n",
    "                'weights': model_weights[weight_key],\n",
    "                'shape': model_weights[weight_key].shape\n",
    "            }\n",
    "            \n",
    "            if bias_key in model_weights:\n",
    "                geometry_layers[f'layer_{i}']['bias'] = model_weights[bias_key]\n",
    "    \n",
    "    # Extract view-dependent MLP weights\n",
    "    for i in range(CONFIG['mlp']['num_layers']):\n",
    "        weight_key = f'_orig_mod.view_mlp.net.{i}.weight'\n",
    "        bias_key = f'_orig_mod.view_mlp.net.{i}.bias'\n",
    "        \n",
    "        if weight_key in model_weights:\n",
    "            view_mlp_layers[f'layer_{i}'] = {\n",
    "                'weights': model_weights[weight_key],\n",
    "                'shape': model_weights[weight_key].shape\n",
    "            }\n",
    "            \n",
    "            if bias_key in model_weights:\n",
    "                view_mlp_layers[f'layer_{i}']['bias'] = model_weights[bias_key]\n",
    "    \n",
    "    return {\n",
    "        'geometry_mlp': geometry_layers,\n",
    "        'view_mlp': view_mlp_layers\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "mlp_weights = extract_mlp_weights(nerf)\n",
    "# Print number of layers and details\n",
    "print(f\"Geometry MLP layers: {len(mlp_weights['geometry_mlp'])}\")\n",
    "print(f\"View MLP layers: {len(mlp_weights['view_mlp'])}\")\n",
    "\n",
    "# Print shape of each layer in the geometry MLP\n",
    "print(\"\\nGeometry MLP layer shapes:\")\n",
    "for layer_name, layer_data in mlp_weights['geometry_mlp'].items():\n",
    "    print(f\"  {layer_name}: {layer_data['shape']} - Input: {layer_data['shape'][1]}, Output: {layer_data['shape'][0]}\")\n",
    "\n",
    "# Print shape of each layer in the view MLP\n",
    "print(\"\\nView MLP layer shapes:\")\n",
    "for layer_name, layer_data in mlp_weights['view_mlp'].items():\n",
    "    print(f\"  {layer_name}: {layer_data['shape']} - Input: {layer_data['shape'][1]}, Output: {layer_data['shape'][0]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "objaverse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
